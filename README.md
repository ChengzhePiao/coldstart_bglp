## Paper Title ##
**Privacy Preserved Blood Glucose Level Cross-Prediction: an Asynchronous Decentralized Federated Learning Approach**
Chengzhe Piao, Taiyu Zhu, Yu Wang, Stephanie E Baldeweg, Paul Taylor, Pantelis Georgiou, Jiahao Sun, Jun Wang, Kezhi Li
## Datasets ##
We introduce four datasets to our experiments, i.e., OhioT1DM, ABC4D (NCT02053051), CTR3 (NCT02137512) and REPLACE-BG (NCT02258373).

All datasets can be accessed publicly apart from ABC4D, which can be accessed via authorised procedures by contacting the project manager and the corresponding author. ABC4D was collected following applicable legal standards, thereby eliminating the need for additional ethical clearance for this study.

* **OhioT1DM**: Marling, C.; and Bunescu, R. C. 2020. The OhioT1DM Dataset for Blood Glucose Level Prediction: Update 2020. In KDH@ECAI’20, volume 2675, 71–74
* **ABC4D**: T. Zhu, L. Kuang, J. Daniels, P. Herrero, K. Li, and P. Georgiou, “IoMT-enabled real-time blood glucose prediction with deep learning and edge computing,” IEEE Internet Things J., vol. 10, no. 5, pp. 3706–3719, 2023.; M. Reddy, P. Pesl, M. Xenou, C. Toumazou, D. Johnston, P. Georgiou, P. Herrero, and N. Oliver, “Clinical safety and feasibility of the advanced bolus calculator for type 1 diabetes based on case-based reasoning: A
6-week nonrandomized single-arm pilot study,” Diabetes Technology & Therapeutics, vol. 18, no. 8, pp. 487–493, 2016.
* **REPLACE-BG**: G. Aleppo, K. J. Ruedy, T. D. Riddlesworth, D. F. Kruger, A. L. Peters, I. Hirsch, R. M. Bergenstal, E. Toschi, A. J. Ahmann, V. N. Shah, M. R. Rickels, B. W. Bode, A. Philis-Tsimikas, R. Pop-Busui, H. Rodriguez, E. Eyth, A. Bhargava, C. Kollman, R. W. Beck, and R. B. S. Group, “Replace-bg: A randomized trial comparing continuous glucose monitoring with and without routine blood glucose monitoring in adults with well-controlled type 1 diabetes,” Diabetes Care, vol. 40, no. 4, pp. 538–545, 2017.

### Data Preprocessing

After getting these datasets, please run the codes under the folder "/gen_dataset" to preprocess the data. 

### Training and Testing

Please run run_codes.py, then all experiments from exp\_1 to exp\_17 will be run by four times and tested cross different datasets.
* 'exp\_11' : 'LR',
* 'exp\_12' : 'XGBoost',
* 'exp\_1' : 'LSTM',
* 'exp\_13' : 'N-BEATS',
* 'exp\_14' : 'NHiTS',
* 'exp\_3' : 'MAML',
* 'exp\_4' : 'MetaSGD',
* 'exp\_2' : 'FedAvg',
* 'exp\_6' : 'GluADFL(Ring)',
* 'exp\_8' : 'GluADFL(Cluster)',
* 'exp\_5' : 'GluADFL(Random)',
* 'exp\_7' : 'GluADFL(Random) with different asychronous settings',
* 'exp\_9' : 'GluADFL(Ring) with different asychronous settings',
* 'exp\_10' : 'GluADFL(Cluster) with different asychronous settings'.

Besides, the fine-tuning expeiments should run exp\_5/fine\_tuning.py

Then, **all our experiments can be reproduced**.

After that, all tables and figures can be generated by running "merge_seeds.ipynb", "summarize_all_methods.ipynb", "summarize_convergence_within_one_dataset.ipynb", "summarize_fine_tuning.ipynb" and "summarize_trend_within_one_dataset.ipynb".

The results in the paper can also be found in "\summary".

We used the following packages with RTX 3090 Ti to run the codes:
* PyTorch 1.11.0
* Scikit-Learn 1.0.2
* Numpy 1.21.5
* Scipy 1.7.3
* Pandas 1.4.2
